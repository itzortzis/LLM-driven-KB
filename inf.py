# from transformers import AutoModelForCausalLM, AutoTokenizer
# import torch
# from datasets import load_dataset

# model = AutoModelForCausalLM.from_pretrained("tinyllama-finetuned_3000_7", device_map="auto")
# tokenizer = AutoTokenizer.from_pretrained("TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T")



# # ds = load_dataset("GEM/web_nlg", "en", split="test[:5]")
# # dataset = ds
# # prompt = """### Instruction:
# # From the given input produce some 5 tags
# # ### Input:
# # A short article about ingredients, restaurants, and desserts in the context of food.
# # ### Response:"""

# prompt = f"""### Instruction:
# From the given input produce a reasonable RDF triple
# ### Input:
# MRI_scans dataset is generated by ICCS and belongs to SECURED project
# ### Response:"""
# inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
# outputs = model.generate(
#     **inputs,
#     max_new_tokens=100,
#     temperature=0.7,
#     top_p=0.9,
#     do_sample=True
# )
# print(tokenizer.decode(outputs[0], skip_special_tokens=True))


from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

models = {
    "Qwen2.5-7B-Instruct": "Qwen/Qwen2.5-7B-Instruct",
    "Mistral-7B-Instruct-v0.3": "mistralai/Mistral-7B-Instruct-v0.3",
    "DeepSeek-Coder-V2-Instruct": "deepseek-ai/DeepSeek-Coder-V2-Instruct",
    "Phi-3-mini-4k-instruct": "microsoft/Phi-3-mini-4k-instruct",
}

prompt = """### Instruction:
From the given input produce a reasonable RDF triple
### Input:
MRI_scans dataset is generated by ICCS and belongs to SECURED project
### Response:"""

for name, repo in models.items():
    print(f"\n{'='*20}\n{name}\n{'='*20}")
    tokenizer = AutoTokenizer.from_pretrained(repo)
    model = AutoModelForCausalLM.from_pretrained(
        repo,
        device_map="auto",
        torch_dtype=torch.float16,
    )

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(
        **inputs,
        max_new_tokens=120,
        temperature=0.3,
        top_p=0.9,
        pad_token_id=tokenizer.eos_token_id
    )

    text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    response = text.split("### Response:")[-1].strip()
    print(response)
